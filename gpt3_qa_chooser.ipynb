{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Controlling GPT-3 Outputs for Question Answering\n",
        "\n",
        "Here's a quick tutorial on how to use GPT-3 to search through a predefined list of facts and return the best answer to a question."
      ],
      "metadata": {
        "id": "StQfZR4Cehh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installs\n",
        "!pip install sentence-transformers -q\n",
        "!pip install openai -q"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-06T03:19:33.269821Z",
          "iopub.execute_input": "2022-12-06T03:19:33.270278Z",
          "iopub.status.idle": "2022-12-06T03:20:09.242141Z",
          "shell.execute_reply.started": "2022-12-06T03:19:33.270246Z",
          "shell.execute_reply": "2022-12-06T03:20:09.240670Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "cellView": "form",
        "id": "X3dQ8MRKNJMQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch \n",
        "import pandas as pd\n",
        "import os\n",
        "import openai\n",
        "import spacy"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-12-06T03:20:56.335569Z",
          "iopub.execute_input": "2022-12-06T03:20:56.336131Z",
          "iopub.status.idle": "2022-12-06T03:20:57.564862Z",
          "shell.execute_reply.started": "2022-12-06T03:20:56.336090Z",
          "shell.execute_reply": "2022-12-06T03:20:57.563827Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "cellView": "form",
        "id": "WaLp5DmdNJMh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Models and Connecting to OpenAI\n",
        "\n",
        "Below we load and define models for breaking text up into sentences (spacy) and searching through these sentences (sentence transformers. In order to connect to GPT-3, you'll need to set up an account with OpenAI and get your API key [here](https://beta.openai.com/account/api-keys)"
      ],
      "metadata": {
        "id": "78iloOEZgWyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embmodel = SentenceTransformer('msmarco-MiniLM-L-6-v3').to(device)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "openai.api_key = \"YOUR KEY HERE\""
      ],
      "metadata": {
        "id": "B_ORyZ-Gf4Wt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating a List of Sentences to be Used as Answers\n",
        "\n",
        "This is a predefined list of facts that will be provided to the chooser to use as its responses to user queries."
      ],
      "metadata": {
        "id": "vc9qn-Diibks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "biography = \"\"\"\n",
        "My name is Henry Leonardi.\n",
        "I am a senior at Ohio State University.\n",
        "I am studying linguistics.\n",
        "I speak spanish and italian fluently.\n",
        "I am from Cincinnati Ohio.\n",
        "I am a fourth year student studying linguistics and minoring in computer information systems.\n",
        "I have 3 years of academic research experience in the field of Natural Language Processing (NLP).\n",
        "I interned at Kyndi and currently works part-time as an NLP Engineer at Holocron Technologies.\n",
        "At Kyndi, I helped automate data annotation tasks and is currently leading a project using GPT-3.\n",
        "At Holocron, I trained and implemented text classification models and has added semantic search capabilities to their database.\n",
        "I'm interested in machine learning and dialogue systems.\n",
        "I'm excited about the potential of NLP and machine learning technologies.\n",
        "\"\"\"\n",
        "\n",
        "# This line breaks the text above into sentences which will be used as answers to user queries\n",
        "facts = [str(sent) for sent in nlp(biography.replace(\"\\n\",\"\")).sents]\n",
        "fact_embs = embmodel.encode(facts)"
      ],
      "metadata": {
        "id": "bSugf_CMijCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using a Query to Find Relevant Answers "
      ],
      "metadata": {
        "id": "XnNl7MVwhIO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relevant_answers(query, label_embs, answers):\n",
        "\n",
        "  #Embed the query using a sentence transformers model\n",
        "  query_embedding = embmodel.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "  #Find the sentences in the dataset most similar to the query\n",
        "  cos_scores = util.cos_sim(query_embedding, label_embs)[0]\n",
        "  top_results = torch.topk(cos_scores, k=5)\n",
        "  top_answers = [answers[i] if i in range(len(answers)) else answers[i-len(answers)] for i in top_results[1]]\n",
        "\n",
        "  #Create a dictionary with the top answers and return it\n",
        "  answer_dict = {}\n",
        "  for i in range(len(top_answers)+1):\n",
        "      if i == len(top_answers):\n",
        "          # This will be a valid answer if the user's query does not have an answer in the fact dataset\n",
        "          answer_dict[i] = \"I don't have a good response to that question\"\n",
        "      else:\n",
        "          answer_dict[i] = top_answers[i]\n",
        "  return answer_dict"
      ],
      "metadata": {
        "id": "L-eGplQyhuZs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using GPT-3 to Choose the Best Answer"
      ],
      "metadata": {
        "id": "eGr6B2Ufi3Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_answer(query, label_embs, answers):\n",
        "\n",
        "    #Get the top answers\n",
        "    answer_dict = relevant_answers(query, label_embs, answers)\n",
        "\n",
        "    #Prompt GPT-3 with the top answers instructing it to return the index of the best one\n",
        "    response = openai.Completion.create(\n",
        "      model=\"text-davinci-003\",\n",
        "      prompt=f\"Here is a dictionary with the answers and their number labels:\\n\\n{str(answer_dict)}\\n\\nThe chatbot takes user queries as inputs, and returns the number label of the answer that will help them the most (the answer doesn't always have to be a perfect match). \\n\\nUser Input: \\\"{query}\\\"\\nChatbot's returned number label: \",\n",
        "      temperature=0.7,\n",
        "      max_tokens=1,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0\n",
        "    )\n",
        "    try:\n",
        "        num_label = int(response[\"choices\"][0][\"text\"])\n",
        "        answer = answer_dict[num_label]                                      \n",
        "    except:\n",
        "        answer = \"Sorry, I don't have a good response for that\"\n",
        "    return answer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-06T03:17:48.872767Z",
          "iopub.execute_input": "2022-12-06T03:17:48.874129Z",
          "iopub.status.idle": "2022-12-06T03:17:48.883683Z",
          "shell.execute_reply.started": "2022-12-06T03:17:48.874080Z",
          "shell.execute_reply": "2022-12-06T03:17:48.882516Z"
        },
        "trusted": true,
        "id": "lrpxBeaHNJMq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_answer(\"what are your interests?\", fact_embs, facts)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-06T03:55:20.385129Z",
          "iopub.execute_input": "2022-12-06T03:55:20.386028Z",
          "iopub.status.idle": "2022-12-06T03:55:20.783371Z",
          "shell.execute_reply.started": "2022-12-06T03:55:20.385987Z",
          "shell.execute_reply": "2022-12-06T03:55:20.782060Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q7m_Q40MNJM2",
        "outputId": "2c9c40a8-da31-4a51-8a9d-1ee243d262cc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm interested in machine learning and dialogue systems.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}